{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9495ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray as rxa\n",
    "from rasterio.enums import Resampling\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/zmhoppinen/Documents/nisar_swe/projects/reference_points')\n",
    "from swe_retrieval import guneriussen_phase_from_depth, guneriussen_depth_from_phase, density_to_permittivity, rho_250\n",
    "from resolution import coarsen_to_resolution\n",
    "from constants import NISAR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac39929",
   "metadata": {},
   "outputs": [],
   "source": [
    "nisar = NISAR()\n",
    "\n",
    "target_resolution = nisar.multi_look_resolution # match nisar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a929930f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "sd_fps = sorted(list(Path('/Users/zmhoppinen/Documents/nisar_swe/data/lidar/nani').glob('*_SNOWDEPTH.tif')))\n",
    "print(len(sd_fps))\n",
    "sds = []\n",
    "for i, fp in enumerate(sd_fps):\n",
    "    # drop Feb 13 2024 due to big gaps\n",
    "    if fp.stem.split('_')[1] == '20240213': continue\n",
    "    da = xr.open_dataarray(fp).expand_dims(time = [datetime.strptime(fp.stem.split('_')[1], '%Y%m%d')]).squeeze('band', drop = True)\n",
    "    sds.append(da)\n",
    "\n",
    "for year in range(2021, 2025):\n",
    "    sds.append(xr.zeros_like(sds[0]).isel(time = 0).expand_dims(time = [datetime.strptime(f'{year}1001', '%Y%m%d')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a99f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mores = xr.concat(sds, 'time').sortby('time')#[:, ::20, ::20]\n",
    "# mores = mores.where(mores.isnull().sum('time') < 16)\n",
    "mores = mores.where(mores >= 0)\n",
    "\n",
    "inc = xr.open_dataarray('/Users/zmhoppinen/Documents/nisar_swe/data/uavsar/generated/052/inc_mores.tif')[0].rio.reproject_match(mores[0])\n",
    "\n",
    "mores_coarse = coarsen_to_resolution(mores, res = target_resolution)\n",
    "inc_coarse = np.deg2rad(coarsen_to_resolution(inc, res = target_resolution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d1dbeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "unw = guneriussen_phase_from_depth(mores_coarse.diff('time'), epsilon = rho_250, inc = inc_coarse)\n",
    "\n",
    "wrapped = xr.zeros_like(unw)\n",
    "wrapped.data = np.angle(np.exp(1j * unw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1e28ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean diff: -1.0046377165048572e-09\n",
      "Max abs diff: 1.3218988357266426e-06\n"
     ]
    }
   ],
   "source": [
    "phase = guneriussen_phase_from_depth(mores_coarse.diff('time'), epsilon=rho_250, inc=inc_coarse)\n",
    "recovered = guneriussen_depth_from_phase(phase, rho_250, inc=inc_coarse)\n",
    "\n",
    "print(\"Mean diff:\", np.nanmean(mores_coarse.diff('time').values - recovered.values))\n",
    "print(\"Max abs diff:\", np.nanmax(np.abs(mores_coarse.diff('time').values - recovered.values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6816f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = xr.open_dataarray('/Users/zmhoppinen/Documents/nisar_swe/data/uavsar/generated/052/tau_2x8.tif')[0].rio.reproject_match(mores[0])\n",
    "ginf = xr.open_dataarray('/Users/zmhoppinen/Documents/nisar_swe/data/uavsar/generated/052/ginf_2x8.tif')[0].rio.reproject_match(mores[0])\n",
    "\n",
    "r2 = xr.open_dataarray('/Users/zmhoppinen/Documents/nisar_swe/data/uavsar/generated/052/r2_2x8.tif')[0].rio.reproject_match(mores[0])\n",
    "\n",
    "tau_coarse = coarsen_to_resolution(tau, res = target_resolution)\n",
    "ginf_coarse = coarsen_to_resolution(ginf, res = target_resolution)\n",
    "r2_coarse = coarsen_to_resolution(r2, res = target_resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73df24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = mores_coarse.to_dataset(name = 'sds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99dfbe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['int'] = wrapped\n",
    "ds['unw'] = unw\n",
    "ds['inc'] = inc_coarse\n",
    "ds['tau'] = tau_coarse\n",
    "ds['ginf'] = ginf_coarse\n",
    "ds['r2'] = r2_coarse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c16fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.assign_coords(ref_time = (\"time\", np.concatenate([[np.datetime64('NaT')], ds.time.data[:-1]])))\n",
    "ds = ds.assign_coords(sec_time = (\"time\", np.concatenate([[np.datetime64('NaT')], ds.time.data[1:]])))\n",
    "\n",
    "# make first (would be nan since no difference) 12 day instead to match nisar\n",
    "dt = np.concatenate([[12], [pd.Timedelta(i).days for i in ds.time.data[1:] - ds.time.data[:-1]]])\n",
    "ds = ds.assign_coords(temp_baseline = (\"time\", dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cba8cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "def coherence_model(dt, g_inf, tau):\n",
    "    return (1 - g_inf) * np.exp(-dt / tau) + g_inf\n",
    "\n",
    "# Apply across the dataset\n",
    "gamma_model = xr.apply_ufunc(\n",
    "    coherence_model,\n",
    "    ds[\"temp_baseline\"],       # (time,)\n",
    "    ds[\"ginf\"],    # (y, x)\n",
    "    ds[\"tau\"],      # (y, x)\n",
    "    input_core_dims=[[\"time\"], [], []],  # dt varies along time\n",
    "    output_core_dims=[[\"time\"]],\n",
    "    vectorize=True,\n",
    "    dask=\"parallelized\",  # allows lazy/dask if dataset is big\n",
    "    output_dtypes=[float],\n",
    ")\n",
    "\n",
    "# Save result back into dataset\n",
    "ds[\"coherence\"] = gamma_model.transpose('time', 'y', 'x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68bcc099",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['diff'] = ds['sds'].diff('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc4188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "def fit_predict_12day(dsd: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    For each pixel (y, x), fit a linear regression of dsd vs temporal_baseline,\n",
    "    excluding baselines > 90 days and times when the total snow decreased then predict snow depth change at 12 days.\n",
    "    Finally mask pixels that have more than 3 nans in the total stack.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dsd : xr.DataArray\n",
    "        Dimensions: (time, y, x)\n",
    "        Coordinates: temporal_baseline (on 'time')\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xr.DataArray\n",
    "        Dimensions: (y, x)\n",
    "        Predicted dsd at 12 days.\n",
    "    \"\"\"\n",
    "    # mask out > 60 day baselines\n",
    "    mask = dsd['temp_baseline'] <= 90\n",
    "    dsd = dsd.where(mask, drop=True)\n",
    "\n",
    "    mask = dsd.mean(['x','y']) >= 0\n",
    "    dsd = dsd.where(mask, drop=True)\n",
    "    \n",
    "    # extract the baseline values\n",
    "    tb = dsd['temp_baseline']\n",
    "    print(len(tb))\n",
    "    \n",
    "    # function to apply per pixel\n",
    "    def linregress_pixel(yvals, tbvals):\n",
    "        # drop NaNs\n",
    "        m = np.isfinite(yvals) & np.isfinite(tbvals)\n",
    "        if m.sum() < 2:  # not enough points to fit\n",
    "            return np.nan\n",
    "        coeffs = np.polyfit(tbvals[m], yvals[m], 1)  # slope, intercept\n",
    "        return np.polyval(coeffs, 12.0)  # prediction at 12 days\n",
    "\n",
    "    # use apply_ufunc for pixelwise regression\n",
    "    result = xr.apply_ufunc(\n",
    "        linregress_pixel,\n",
    "        dsd, tb,\n",
    "        input_core_dims=[[\"time\"], [\"time\"]],\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "        output_dtypes=[float],\n",
    "    )\n",
    "    \n",
    "    result.name = \"dsd_12day\"\n",
    "    result.attrs[\"description\"] = \"Predicted snow depth change at 12 days from linear fit\"\n",
    "\n",
    "    result = result.where(dsd.isnull().sum('time')<3)\n",
    "    \n",
    "    return result\n",
    "\n",
    "ds['dsd_12'] = fit_predict_12day(ds['diff'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "671af60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf(f'/Users/zmhoppinen/Documents/nisar_swe/data/reference_point/full_{target_resolution}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91afbaad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geog6655_coherence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
