{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb42e0a",
   "metadata": {},
   "source": [
    "# Lowman Coherence\n",
    "\n",
    "This is the start of my exploration of what coherence looks like for the Lowman site in Idaho. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ae277a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julo9057/anaconda3/envs/geog6655_coherence/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray as rxa\n",
    "from rasterio.enums import Resampling\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tif\n",
    "\n",
    "from uavsar_pytools.convert.tiff_conversion import read_annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823d34d",
   "metadata": {},
   "source": [
    "Import my custom modules. \n",
    "I had to import specific functions from my files, otherwise I was getting an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "218ebb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "scripts_dir = (os.path.dirname(os.getcwd()) + '/scripts/')\n",
    "sys.path.append(scripts_dir)\n",
    "\n",
    "from plotting import plot_tifs_grid \n",
    "from coherence import calc_coherence_unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75e45c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/data/snowex_lowman/slcs/\"\n",
    "in_dir = Path(data_dir)\n",
    "slcs_05208 = sorted(list(in_dir.glob('*05208*.slc')))\n",
    "anns_05208 = sorted(list(in_dir.glob('*05208*.ann')))\n",
    "slcs_23205 = sorted(list(in_dir.glob('*23205*.slc')))\n",
    "anns_23205 = sorted(list(in_dir.glob('*23205*.ann')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c27b4a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/data/snowex_lowman/slcs/lowman_05208_20002_006_200131_L090VV_01_BU_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_05208_20007_002_200213_L090VV_01_BU_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_05208_20011_002_200221_L090VV_01_BU_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_05208_20016_003_200311_L090VV_01_BU_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_05208_21009_005_210203_L090VV_01_BU_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_05208_21012_004_210210_L090VV_01_BU_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_05208_21015_009_210303_L090VV_01_BU_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_05208_21017_019_210310_L090VV_01_BU_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_05208_21019_019_210316_L090VV_01_BU_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_05208_21021_007_210322_L090VV_01_BU_s2_2x8.slc\n"
     ]
    }
   ],
   "source": [
    "d5 = read_annotation(anns_05208[0])\n",
    "d5 = pd.DataFrame(d5).T\n",
    "rows, cols = d5.loc['slc_2_2x8 rows', 'value'], d5.loc['slc_2_2x8 columns', 'value']\n",
    "stack5 = np.zeros((len(slcs_05208), rows, cols), '<c8')\n",
    "dates5 = []\n",
    "for i, (slc, ann) in enumerate(zip(slcs_05208, anns_05208, strict = True)):\n",
    "    print(slc)\n",
    "    d = read_annotation(ann)\n",
    "    d = pd.DataFrame(d).T\n",
    "    timestamp = d.loc['start time of acquisition', 'value']\n",
    "\n",
    "    \n",
    "    stack5[i, :, :] = np.fromfile(slc, '<c8').reshape(rows, cols)\n",
    "    dates5.append(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "295132ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/data/snowex_lowman/slcs/lowman_23205_20002_007_200131_L090VV_01_BC_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_23205_20007_003_200213_L090VV_01_BC_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_23205_20011_003_200221_L090VV_01_BC_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_23205_20016_004_200311_L090VV_01_BC_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_23205_21002_004_210115_L090VV_01_BC_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_23205_21004_003_210120_L090VV_01_BC_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_23205_21008_000_210127_L090VV_01_BC_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_23205_21009_004_210203_L090VV_01_BC_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_23205_21012_000_210210_L090VV_01_BC_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_23205_21015_008_210303_L090VV_01_BC_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_23205_21017_018_210310_L090VV_01_BC_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_23205_21019_018_210316_L090VV_01_BC_s2_2x8.slc\n",
      "../data/data/snowex_lowman/slcs/lowman_23205_21021_006_210322_L090VV_01_BC_s2_2x8.slc\n"
     ]
    }
   ],
   "source": [
    "d2 = read_annotation(anns_23205[0])\n",
    "d2 = pd.DataFrame(d2).T\n",
    "rows, cols = d2.loc['slc_2_2x8 rows', 'value'], d2.loc['slc_2_2x8 columns', 'value']\n",
    "stack2 = np.zeros((len(slcs_23205), rows, cols), '<c8')\n",
    "dates2 = []\n",
    "for i, (slc, ann) in enumerate(zip(slcs_23205, anns_23205, strict = True)):\n",
    "    print(slc)\n",
    "    d = read_annotation(ann)\n",
    "    d = pd.DataFrame(d).T\n",
    "    timestamp = d.loc['start time of acquisition', 'value']\n",
    "\n",
    "    \n",
    "    stack2[i, :, :] = np.fromfile(slc, '<c8').reshape(rows, cols)\n",
    "    dates2.append(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37c0cfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(stack5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691dae11",
   "metadata": {},
   "source": [
    "Calculaing the coherence in a loop for the 10 lowman 05208 files took 30 seconds per pair for 45 total pair combinations. After doing this once, I saved all of those output into one big .tif file with 45 layers. \n",
    "\n",
    "This file is 14 GB but loads pretty quickly using the tifffile module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b961a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c5 = []\n",
    "# for i in range(len(stack5)) : \n",
    "#     for j in range(len(stack5)) : \n",
    "#         if (i >= j) : \n",
    "#             continue\n",
    "#         start_time = time()\n",
    "#         print(f\"calculating coherence for img {i} and img {j}\")\n",
    "#         c5.append(calc_coherence_unweighted(stack5[i], stack5[j]))\n",
    "#         end_time = time()\n",
    "#         elapsed_time = end_time - start_time\n",
    "#         print(f\"   elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "### OR \n",
    "c5 = tif.imread(data_dir + 'lowman_05208_coherences.stack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84cab646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 8333, 4896)\n"
     ]
    }
   ],
   "source": [
    "print(c5.shape)\n",
    "# tif.imwrite(data_dir + 'lowman_05208_coherences.stack', c5, bigtiff=True)\n",
    "# c5.tofile(data_dir + '../lowman_05208_coherences.stack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9ddeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tifs_grid(c5[0:9, :, :], is_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c818cc83",
   "metadata": {},
   "source": [
    "At this stage, I should have a stack of coherences between every image pair. For all 10 images, this will be \n",
    "\n",
    "Next I want to calculate average coherence for each image pair and make it into a coherence matrix, like Zach suggests. \n",
    "\n",
    "Then I want to "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geog6655_coherence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
